from fastapi import FastAPI, Query
import pickle
from tensorflow.keras.models import load_model
import re
import spacy
from spacy.lang.es.stop_words import STOP_WORDS
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Cargar el tokenizador
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)
print("Tokenizador cargado desde 'tokenizer.pickle'")

# Cargar el modelo entrenado
model_path = 'saved_models/lstm_model.h5'
model = load_model(model_path)
print(f"Modelo cargado desde {model_path}")

# Inicializar spaCy en espaÃ±ol
nlp = spacy.load("es_core_news_sm")

# Diccionario de emojis especÃ­ficos y sus significados en espaÃ±ol
emoji_dict = {
    'ðŸ˜‚': 'risa', 'ðŸ‘': 'aplauso', 'â¤': 'amor', 'âœ¨': 'brillo', 'ðŸ¤£': 'carcajada',
    'ðŸ™Œ': 'apoyo', 'ðŸŒŸ': 'estrella', 'ðŸ˜¢': 'tristeza', 'ðŸ’«': 'resplandor', 'ðŸ”¥': 'fuego',
    'ðŸ˜': 'amor', 'ðŸ¤®': 'asco', 'ðŸ’ª': 'fuerza', 'ðŸ™„': 'recelo', 'ðŸ™': 'rezar',
    'ðŸ˜®': 'sorpresa', 'ðŸ˜­': 'llanto', 'ðŸŽ¨': 'arte', 'ðŸŒˆ': 'arcoiris', 'ðŸ˜¡': 'enfado',
    'ðŸ“¸': 'fotografÃ­a', 'ðŸ’©': 'mierda', 'ðŸŽ¯': 'objetivo', 'ðŸš€': 'impulso', 'ðŸ¤·': 'indiferencia',
    'ðŸ¤¡': 'payaso', 'â­': 'estrella', 'ðŸ˜˜': 'beso', 'ðŸ³ï¸': 'bandera', 'ðŸ¤¦': 'frustraciÃ³n',
    'âœŠ': 'puÃ±o', 'ðŸ˜”': 'decepciÃ³n', 'ðŸ‘Œ': 'ok', 'ðŸ’¯': 'perfecto', 'ðŸ’œ': 'corazÃ³n morado',
    'ðŸ‘': 'bien', 'ðŸŒ±': 'crecimiento', 'ðŸ˜Ž': 'genial', 'ðŸ’€': 'calavera', 'ðŸ˜…': 'alivio',
    'ðŸ¤”': 'pensando', 'ðŸ¤‘': 'dinero', 'ðŸ’•': 'amor', 'ðŸ’': 'amor'
}

# FunciÃ³n de preprocesamiento
def preprocess_text(text):
    text = text.lower()
    for emoji_char, emoji_desc in emoji_dict.items():
        text = text.replace(emoji_char, emoji_desc)
    text = re.sub(r":(\w+):", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#\w+", "", text)
    text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
    text = re.sub(r"[^\w\s]", "", text)
    
    additional_stopwords = {"el", "la", 'los', 'las', 'unos', 'unas', "un", "una", "lo", "que"}
    stopwords = STOP_WORDS.union(additional_stopwords)
    
    doc = nlp(text)
    tokens = [
        token.lemma_ for token in doc
        if token.lemma_ not in stopwords and not token.is_punct and not token.is_space and len(token.lemma_) > 3
    ]
    return " ".join(tokens)

def preprocess_input(text, tokenizer, max_length):
    text = preprocess_text(text)
    sequence = tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=max_length)
    return padded_sequence

# Crear una instancia de FastAPI
app = FastAPI()

# Definir el endpoint para la predicciÃ³n
@app.get("/predict/")
async def predict(texto: str = Query(..., description="Texto a clasificar")):
    processed_input = preprocess_input(texto, tokenizer, max_length=100)  # Ajusta max_length segÃºn tu modelo
    prediction = model.predict(processed_input)
    prediction_class = (prediction > 0.5).astype(int)
    return {"Etiqueta": int(prediction_class[0][0])}
